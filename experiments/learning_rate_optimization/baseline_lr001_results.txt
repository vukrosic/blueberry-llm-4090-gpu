ğŸ” Device: CUDA
GPU: NVIDIA GeForce RTX 4090
Memory: 25.3 GB
ğŸŒ± Set all seeds to 42
ğŸ“¦ Loading cached data from data_cache/tokenized_data_2000_500000.pkl
âœ… Loaded 2000 documents, 500,000 tokens from cache
ğŸ“Š Dataset: 449540 train, 49948 val samples

============================================================
ğŸ§ª TRAINING: Mixture of Experts Model
============================================================

ğŸ“‹ MoE Model Configuration:
   Architecture: 384d, 6L, 8H, 1536ff
   MoE: 8 experts, top-2 routing
   Training: 20 steps, batch size 24
   Data: 500,000 tokens, seq_len 512

ğŸš€ Training MoE model with 8 experts (top-2)
ğŸŒ± Set all seeds to 42
  ğŸ“Š Total parameters: 79,059,840
  ğŸ“Š Active parameters: 22,436,736
  ğŸ“Š Expert parameters: 56,623,104
  ğŸ“Š Parameter efficiency: 28.4% active per forward pass
  Muon parameters: 60,180,480
  AdamW parameters: 18,879,360
Training MoE:   0%|          | 0/20 [00:00<?, ?it/s]Training MoE:   0%|          | 0/20 [00:00<?, ?it/s, loss=10.7965, aux=0.0602, acc=0.017, ppl=48850.9]
Step 10: Val Loss: 10.5374, Val Acc: 0.0157, Val PPL: 37698.54
Training MoE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.56it/s, loss=10.7965, aux=0.0602, acc=0.017, ppl=48850.9]Training MoE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.54it/s, loss=10.7965, aux=0.0602, acc=0.017, ppl=48850.9]

ğŸ“Š Final Results:
   Val Loss: 9.5670
   Val Accuracy: 0.0157
   Val Perplexity: 14286.21

ğŸ“ˆ Evaluation loss plot saved as 'eval_loss_vs_time.png'

ğŸ¯ MoE Model Results:
â±ï¸ Training time: 0.2 minutes
ğŸ† Final Results:
   Validation Loss: 9.5670
   Validation Accuracy: 0.0157
   Validation Perplexity: 14286.21
============================================================
